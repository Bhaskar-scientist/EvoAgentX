import yaml
from copy import deepcopy
from abc import ABC, abstractmethod
from pydantic import Field
from typing import Union, Optional, Type, List

from ..core.parser import Parser
from ..core.module_utils import parse_json_from_text
from ..core.registry import register_model
from .model_configs import LLMConfig, LLMGenerationConfig, APILLMConfig, LocalLLMConfig


class LLMOutputParser(Parser):

    """
    a basic LLM output parser that directly store the text generated by LLM in the .text attribute.
    """
    content: str = Field(default=None, exclude=True, description="the text generated by LLM")

    @classmethod
    def get_content_data(self, content: str, **kwargs) -> dict:

        extracted_json_list = parse_json_from_text(content)
        if len(extracted_json_list) > 0:
            json_str = extracted_json_list[0] # only use the first JSON
            data = yaml.safe_load(json_str)
        else:
            data = {}
        return data
    
    @classmethod
    def parse(cls, content: str, **kwargs):
        """
        the method used to parse text generated by LLM. This method only passes the input text to the .text attribute. 
        """
        if not isinstance(content, str):
            assert f"The input to {cls.__name__}.parse should be a str, but found {type(content)}."
        data = cls.get_content_data(content, **kwargs)
        parser = cls.from_dict(data, **kwargs)
        parser.content = content
        return parser
    
    def to_str(self, **kwargs) -> str:
        return self.content


class BaseLLM(ABC):
    
    def __init__(self, config: LLMConfig, **kwargs):

        self.config = config
        self.kwargs = kwargs
        self.init_model()
    
    @abstractmethod
    def init_model(self):
        pass

    def get_generation_config(self, generation_config: LLMGenerationConfig=None, **kwargs) -> LLMGenerationConfig:

        """
        obtain generation config.

        By default, it will return self.config.generation_config. If a generation_config is passed as input, replace the value 
            in self.config.generation_config with values in the generation_config. 
            Moreover, if the kwargs contains some generation_config variables, use these values to update the self.config.generation_config.

        Args: 
            generation_config (LLMGenerationConfig): the input generation_config 
        
        Returns:
            LLMGenerationConfig: a new generation_config
        """

        config = deepcopy(self.config.generation_config)
        if generation_config is not None:
            for field_name in generation_config.model_fields:
                value = getattr(generation_config, field_name)
                old_value = getattr(config, field_name)
                if hasattr(config, field_name) and (value is None or (value is not None and type(value) == type(old_value))):
                    setattr(config, field_name, value)
        
        for key, value in kwargs.items():
            old_value = getattr(config, key)
            if hasattr(config, key) and (value is None or (value is not None and type(value)==type(old_value))):
                setattr(config, key, value)
        
        return config

    @abstractmethod
    def single_generate(self, prompt: str, generation_config: LLMGenerationConfig, **kwargs) -> str:
        """
        generate LLM output for a given prompt. 

        Args:
            prompt (str): the input to the LLM.
            generation_config (LLMGenerationConfig): the generation config for LLM.
        
        Returns:
            str: the generated output from LLM.
        """
        pass

    @abstractmethod
    def batch_generate(self, prompt: List[str], generation_config: LLMGenerationConfig, **kwargs) -> List[str]:
        """
        generate outputs for a batch of prompts. 

        Args: 
            prompts (List[str]): a batch of inputs to the LLM. 
            generation_config (LLMGenerationConfig): the generation config for LLM.            
        
        Returns:
            List[str]: a list of generated outputs from LLM.
        """
        pass

    @abstractmethod
    def parse_generated_text(self, text: str, parser: Optional[Type[LLMOutputParser]]=None, **kwargs) -> LLMOutputParser:
        pass

    @abstractmethod
    def parse_generated_texts(self, texts: List[str], parser: Optional[Type[LLMOutputParser]]=None, **kwargs) -> List[LLMOutputParser]:
        pass 
    
    def generate(
        self, 
        prompt: Union[str, List[str]], 
        generation_config: Optional[LLMGenerationConfig]=None,
        parser: Optional[Type[LLMOutputParser]] = None, 
        **kwargs
    ) -> Union[LLMOutputParser, List[LLMOutputParser]]:
        
        """
        generate LLM output(s) for (a) prompt(s) and parsed the result into an LLMOutputParser object. 

        Args:
            prompt (Union[str, List[str]]): the input to the LLM. 
            generation_config (LLMGenerationConfig): the generation config for LLM. If None, self.generation_config will be used by default.
            parser (Optional[Type[LLMOutputParser]]): A LLMOutputParser (sub)class used to parse the LLM output.
                This class should implement .parse() method to parse the output. If None, LLMOutputParser will be used by default.
        """
        pass


class APILLM(BaseLLM):

    def __init__(self, config: APILLMConfig, **kwargs):
        super().__init__(config, **kwargs)


class LocalLLM(BaseLLM):

    def __init__(self, config: LocalLLMConfig, **kwargs):
        super().__init__(config, **kwargs)


########### for debugging #################
class DebugConfig(LocalLLMConfig):

    size: int = Field(default=11)


@register_model(config_cls=DebugConfig, alias=["debug_model"])
class DebugModel(LocalLLMConfig):

    def single_generate(self, prompt: str, generation_config: LLMGenerationConfig, **kwargs) -> str:
        return "single_generate"
    
    def batch_generate(self, prompt: List[str], generation_config: LLMGenerationConfig, **kwargs) -> List[str]:
        return ["batch_generate"]


__all__ = ["LLMConfig", "BaseLLM", "APILLM", "LocalLLM", "DebugConfig", "DebugModel"]


